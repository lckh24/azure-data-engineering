{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53e31907-80fa-4cf7-ad16-73a3fe4d1d72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ab81e9e-1b9a-4969-a0f3-636b74c74b71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from pyspark.sql.functions import col, to_date, month, dayofweek, year\n",
    "import logging\n",
    "\n",
    "def predict_modal_price(df, pipeline, \n",
    "                       date_column: str = 'DATE_OF_PRICING'):\n",
    "    \"\"\"\n",
    "    Predict modal prices using Spark DataFrame operations for feature engineering.\n",
    "    \n",
    "    Args:\n",
    "        df (pyspark.sql.DataFrame): Input Spark DataFrame with pricing data\n",
    "        pipeline (joblib.base.BaseEstimator): Pre-trained prediction model\n",
    "        date_column (str): Name of the date column (default: 'DATE_OF_PRICING')\n",
    "    \n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: DataFrame with predictions and engineered features\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = df.withColumn(date_column, to_date(col(date_column)))\n",
    "\n",
    "        df = df.withColumn('month', month(col(date_column))) \\\n",
    "               .withColumn('dayofweek', dayofweek(col(date_column))) \\\n",
    "               .withColumn('year', year(col(date_column)))\n",
    "        pandas_df = df.toPandas()\n",
    "        pandas_df['MODAL_PRICE_PREDICT'] = pipeline.predict(pandas_df)\n",
    "        result_df = spark.createDataFrame(pandas_df)\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in predict_modal_price: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    try:\n",
    "        pipeline = joblib.load(\"best_price_prediction_model.joblib\")\n",
    "        df = spark.sql(\"\"\"\n",
    "            SELECT * \n",
    "            FROM pricing_analytics.gold.datalake_price_prediction_gold \n",
    "            WHERE DATE_OF_PRICING IS NOT NULL\n",
    "        \"\"\")\n",
    "        df_result = predict_modal_price(df, pipeline)\n",
    "        # Save results to Delta table\n",
    "        df_result.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .saveAsTable(\"pricing_analytics.gold.datalake_price_predictions\")\n",
    "        \n",
    "        return df_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in main: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "df_result = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2585e073-d28f-468d-8d1d-602bf4a29bdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_result)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Apply-Model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
