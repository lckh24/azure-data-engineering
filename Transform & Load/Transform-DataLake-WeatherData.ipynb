{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47197e6d-2f22-4338-b387-619cef5dd5bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "WeatherData JSON Source File Path : \"abfss://bronze@datalakestorageaccountname.dfs.core.windows.net/weather-data/\n",
    "\"\n",
    "\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.join.html#pyspark.sql.DataFrame.join\" target=\"_blank\">**DataFrame Joins** </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35ed42af-11ed-4baa-bb5d-2c786093e3f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 1: Define the Variables to read weather-data ingested in bronze Layer\n",
    "\n",
    "1. Replace <datalakestorageaccountname> with the ADLS account name crated in your account\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b027645-a6db-4e4d-8751-b5817a14061e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weatherDataSourceLayerName = 'bronze'\n",
    "weatherDataSourceStorageAccountName = 'lckudadatalakehousedev'\n",
    "weatherDataSourceFolderName = 'weather-data'\n",
    "\n",
    "weatherDataSourceFolderPath = f\"abfss://{weatherDataSourceLayerName}@{weatherDataSourceStorageAccountName}.dfs.core.windows.net/{weatherDataSourceFolderName}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da601d03-17fb-4bb6-846b-7cfb308e4bd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 2: Create Spark Dataframe For weather-data in Json form stored in bronze layer\n",
    "\n",
    "1. Define Spark Dataframe variable name as weatherDataBronzeDF\n",
    "1. Use spark.read.json method to read the source data path defined above using the variable weatherDataSourceFolderPath \n",
    "1. Include display for converted Spark Dataframe variables to view the dataframe columns and data for further processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef115d39-0cec-42ce-9509-b4fc861f1418",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weatherDataBronzeDF = (spark\n",
    "                       .read\n",
    "                       .json(weatherDataSourceFolderPath))\n",
    "\n",
    "display(weatherDataBronzeDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49108a22-e6c4-417e-8799-e22418fe8179",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 3: Convert Weathe Date Values in ARRAY format to ROWS Using Explode\n",
    "\n",
    "1. Import all functions from pyspark.sql.functions package\n",
    "1. Define New Spark Dataframe variable name as weatherDataDailyDateTransDF\n",
    "1. Use Dataframe select method to select the columns given below from source Spark Dataframe variable weatherDataBronzeDF\n",
    "1. First select column is \"daily.time\" and apply the explode function on this source column and also add alias for exploded values column as \"weatherDate\"\n",
    "1. Along with above explode select the columns \"marketName\" , \"latitude\" , \"longitude\" from source Spark Dataframe\n",
    "1. Last column in the select is running sequence id generated by Spark function monotonically_increasing_id() and add alias name as 'sequenceId'\n",
    "1. Include display for converted Spark Dataframe variables to view the dataframe columns and data for further processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e011f14-d512-4dd5-993a-0c0ff11eec61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "weatherDataDailyDateTransDF = (weatherDataBronzeDF\n",
    "                          .select(\n",
    "                          explode(\"daily.time\").alias(\"weatherDate\")\n",
    "                          ,col(\"marketName\")\n",
    "                          ,col(\"latitude\").alias(\"latitude\")\n",
    "                          ,col(\"longitude\").alias(\"longitude\")\n",
    "                          ,monotonically_increasing_id().alias('sequenceId')\n",
    "                          ))\n",
    "\n",
    "display(weatherDataDailyDateTransDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "768d22fa-6d9c-45cd-a8b5-a0e5a65785bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 4: Convert Maximum Temparature Values in ARRAY format to ROWS Using Explode\n",
    "\n",
    "1. Define New Spark Dataframe variable name as weatherDataMaxTemparatureTransDF\n",
    "1. Use Dataframe select method to select the columns given below from source Spark Dataframe variable weatherDataBronzeDF\n",
    "1. First select column is \"daily.temperature_2m_max\" and apply the explode function on this source column and also add alias for exploded values column as \"maximumTemparature\"\n",
    "1. Along with above explode select the columns \"marketName\" , \"latitude\" , \"longitude\" from source Spark Dataframe\n",
    "1. Last column in the select is running sequence id generated by Spark function monotonically_increasing_id() and add alias name as 'sequenceId'\n",
    "1. Add one more column from the Source Spark Dataframe \"daily_units.temperature_2m_max\" and provide alias name as \"unitOfTemparature\"\n",
    "1. Include display for converted Spark Dataframe variables to view the dataframe columns and data for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1826e61f-889a-4a24-98b8-4ab7efaa709a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weatherDataMaxTemparatureTransDF = (weatherDataBronzeDF\n",
    "                          .select(\n",
    "                          explode(\"daily.temperature_2m_max\").alias(\"maximumTemparature\")\n",
    "                          ,col(\"marketName\")\n",
    "                          ,col(\"latitude\").alias(\"latitude\")\n",
    "                          ,col(\"longitude\").alias(\"longitude\")\n",
    "                          ,monotonically_increasing_id().alias('sequenceId')\n",
    "                          ,col(\"daily_units.temperature_2m_max\").alias(\"unitOfTemparature\")\n",
    "\n",
    "                          ))\n",
    "\n",
    "display(weatherDataMaxTemparatureTransDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7899958-0284-4f3f-a5c8-5ba85c59b357",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 5: Convert Minimum Temparature Values in ARRAY format to ROWS Using Explode\n",
    "\n",
    "1. Define New Spark Dataframe variable name as weatherDataMinTemparatureTransDF\n",
    "1. Use Dataframe select method to select the columns given below from source Spark Dataframe variable weatherDataBronzeDF\n",
    "1. First select column is \"daily.temperature_2m_min\" and apply the explode function on this source column and also add alias for exploded values column as \"minimumTemparature\"\n",
    "1. Along with above explode select the columns \"marketName\" , \"latitude\" , \"longitude\" from source Spark Dataframe\n",
    "1. Last column in the select is running sequence id generated by Spark function monotonically_increasing_id() and add alias name as 'sequenceId'\n",
    "1. Include display for converted Spark Dataframe variables to view the dataframe columns and data for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7526fdc8-5ca2-444c-8449-4f0d2630ef45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weatherDataMinTemparatureTransDF = (weatherDataBronzeDF\n",
    "                          .select(\n",
    "                          explode(\"daily.temperature_2m_min\").alias(\"minimumTemparature\")\n",
    "                          ,col(\"marketName\")\n",
    "                          ,col(\"latitude\").alias(\"latitude\")\n",
    "                          ,col(\"longitude\").alias(\"longitude\")                          \n",
    "                          ,monotonically_increasing_id().alias('sequenceId')\n",
    "\n",
    "                          ))\n",
    "\n",
    "display(weatherDataMinTemparatureTransDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8af1dac9-79a9-4bae-a3e4-a6891b09fa47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 6: Convert Rain Fall Values in ARRAY format to ROWS Using Explode\n",
    "\n",
    "1. Define New Spark Dataframe variable name as weatherDataRainFallTransDF\n",
    "1. Use Dataframe select method to select the columns given below from source Spark Dataframe variable weatherDataBronzeDF\n",
    "1. First select column is \"daily.rain_sum\" and apply the explode function on this source column and also add alias for exploded values column as \"rainFall\"\n",
    "1. Along with above explode select the columns \"marketName\" , \"latitude\" , \"longitude\" from source Spark Dataframe\n",
    "1. Last column in the select is running sequence id generated by Spark function monotonically_increasing_id() and add alias name as 'sequenceId'\n",
    "1. Add one more column from the Source Spark Dataframe \"daily_units.rain_sum\" and provide alias name as \"unitOfRainFall\"\n",
    "1. Include display for converted Spark Dataframe variables to view the dataframe columns and data for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9de0d2f3-acc6-4e08-bd08-c7c68bf82b8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weatherDataRainFallTransDF = (weatherDataBronzeDF\n",
    "                          .select(\n",
    "                          explode(\"daily.rain_sum\").alias(\"rainFall\")\n",
    "                          ,col(\"marketName\")\n",
    "                          ,col(\"latitude\").alias(\"latitude\")\n",
    "                          ,col(\"longitude\").alias(\"longitude\")                          \n",
    "                          ,monotonically_increasing_id().alias('sequenceId')\n",
    "                          ,col(\"daily_units.rain_sum\").alias(\"unitOfRainFall\")\n",
    "\n",
    "                          ))\n",
    "\n",
    "display(weatherDataRainFallTransDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62419261-614c-447f-a961-fb9d6c67172d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 7: Join All Intermediate Dataframes To Merge All Data & Write Into Silver Layer\n",
    "\n",
    "1. Define New Spark Dataframe variable name as weatherDataTransDF\n",
    "1. Join weatherDataDailyDateTransDF with weatherDataMaxTemparatureTransDF Using the Joining Columns ['marketName','latitude','longitude','sequenceId']\n",
    "1. Extend weatherDataDailyDateTransDF with weatherDataMinTemparatureTransDF Using the Joining Columns ['marketName','latitude','longitude','sequenceId']\n",
    "1. Extend weatherDataDailyDateTransDF with weatherDataRainFallTransDF Using the Joining Columns ['marketName','latitude','longitude','sequenceId']\n",
    "1. Select the Columns \"marketName\" , \"weatherDate\" , \"unitOfTemparature\" , \"maximumTemparature\" , \"minimumTemparature\" , \"unitOfRainFall\" , \"rainFall\" , \"latitude\" and \"longitude\" to write final output columns into silve layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c6ad1a0-ecff-4357-a62f-661139a35b80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weatherDataTransDF = (weatherDataDailyDateTransDF\n",
    "                      .join(weatherDataMaxTemparatureTransDF, ['marketName','latitude','longitude','sequenceId'])\n",
    "                      .join(weatherDataMinTemparatureTransDF, ['marketName','latitude','longitude','sequenceId'])\n",
    "                      .join(weatherDataRainFallTransDF, ['marketName','latitude','longitude','sequenceId'])\n",
    "                      .select(col(\"marketName\")\n",
    "                              ,col(\"weatherDate\")\n",
    "                              ,col(\"unitOfTemparature\")\n",
    "                              ,col(\"maximumTemparature\")\n",
    "                              ,col(\"minimumTemparature\")\n",
    "                              ,col(\"unitOfRainFall\")\n",
    "                              ,col(\"rainFall\")\n",
    "                              ,col(\"latitude\")\n",
    "                              ,col(\"longitude\"))\n",
    "                     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79f3840f-bd2a-4367-89e4-33ea4d751244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdf = weatherDataTransDF.toPandas()\n",
    "weatherDataTransDF = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "030cbf8f-23b5-452b-8c9f-65eefb19c432",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 8: Write the Final Transformed Dataframe Into Silve Layer As Delta Table\n",
    "\n",
    "1. Write Final Spark Dataframe weatherDataTransDF values using spark.write method\n",
    "1. Use Write mode as overwrite \n",
    "1. Write the data into the Datalake Table \"pricing_analytics.silver.weather_data_silver\" using saveAsTable Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52ea64e3-ece7-4b25-95fa-7a384a44534b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(weatherDataTransDF\n",
    " .write\n",
    " .mode(\"overwrite\")  \n",
    " .saveAsTable(\"pricing_analytics.silver.weather_data_silver\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3aa4b6da-257c-4f84-9951-e5e2db78e315",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 9: Test The Data Stored in Tranformed Silve Layer Table\n",
    "1. Write SELECT query to select the data from pricing_analytics.silver.weather_data_silver table\n",
    "1. Check the data for any one of the Market matches with the source data in Complex JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a476033-8aa4-4c35-962b-af9b8ef71172",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM pricing_analytics.silver.weather_data_silver\").show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 816273167594636,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Transform-DataLake-WeatherData",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
