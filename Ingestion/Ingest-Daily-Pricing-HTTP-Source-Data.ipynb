{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e70a5f60-5223-4d08-b0d9-6644333a6ce4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd \n",
    "from pyspark.sql.functions import current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1745eba4-9b24-44ae-862f-4f62a2e3ca5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# processName = dbutils.widgets.get(\"prm_processName\")\n",
    "# nextSourceFileDateSQL = f\"\"\"SELECT COALESCE(MAX(PROCESSED_FILE_TABLE_DATE)+1, '2023-01-01') AS NEXT_SOURCE_FILE_DATE FROM pricing_analytics.processrunlogs.deltalakehouse_process_runs\n",
    "# WHERE PROCESS_NAME = \"dailyPricingSourceIngest\" AND PROCESS_STATUS = \"Completed\"\n",
    "# \"\"\"\n",
    "# nextSourceFileDateDF = spark.sql(nextSourceFileDateSQL)\n",
    "# nextSourceFileDateDF.show()\n",
    "# nextSourceFileDateDF.select(\"NEXT_SOURCE_FILE_DATE\").collect()[0][\"NEXT_SOURCE_FILE_DATE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38e7cba0-104c-4449-a865-9cf0b69d43b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# nextSourceFileDateDF = spark.sql(nextSourceFileDateSQL)\n",
    "# nextSourceFileDateDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a9bfabd-fa60-4ff9-bc68-a7b426b38dba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "processName = dbutils.widgets.get(\"prm_processName\")\n",
    "nextSourceFileDateSQL = f\"\"\"SELECT COALESCE(MAX(PROCESSED_FILE_TABLE_DATE)+1, '2023-01-01') AS NEXT_SOURCE_FILE_DATE FROM pricing_analytics.processrunlogs.deltalakehouse_process_runs\n",
    "WHERE PROCESS_NAME = \"dailyPricingSourceIngest\" AND PROCESS_STATUS = \"Completed\"\n",
    "\"\"\"\n",
    "nextSourceFileDateDF = spark.sql(nextSourceFileDateSQL)\n",
    "nextSourceFileDateDF.show()\n",
    "nextSourceFileDateDF.select(\"NEXT_SOURCE_FILE_DATE\").collect()[0][\"NEXT_SOURCE_FILE_DATE\"]\n",
    "nextSourceFileDateDF = spark.sql(nextSourceFileDateSQL)\n",
    "nextSourceFileDateDF.show()\n",
    "dailyPricingSourceBaseURL = \"https://retailpricing.blob.core.windows.net/\"\n",
    "dailyPricingSubFolder = \"daily-pricing/\"\n",
    "dailyPricingSourceFileDate = datetime.strptime(str(nextSourceFileDateDF.select(\"NEXT_SOURCE_FILE_DATE\").collect()[0][\"NEXT_SOURCE_FILE_DATE\"]), \"%Y-%m-%d\").strftime('%m%d%Y')\n",
    "dailyPricingSourceFileName = f\"PW_MW_DR_{dailyPricingSourceFileDate}.csv\"\n",
    "\n",
    "dailyPricingSinkLayerName = \"bronze\"\n",
    "dailyPricingStorageAccountName = \"lckudadatalakehousedev\"\n",
    "dailyPricingSinkFolderName = \"daily-pricing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec0d8f1b-0f12-4fe6-9a91-5b4535bb25f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from datetime import datetime, timedelta\n",
    "# from pyspark.sql.functions import current_timestamp\n",
    "# import pandas as pd\n",
    "\n",
    "# # Initialize variables\n",
    "# processName = dbutils.widgets.get(\"prm_processName\")\n",
    "# end_date = datetime.strptime(\"2023-06-30\", \"%Y-%m-%d\")\n",
    "\n",
    "# # Get the initial next source file date\n",
    "# nextSourceFileDateSQL = f\"\"\"SELECT COALESCE(MAX(PROCESSED_FILE_TABLE_DATE)+1, '2023-01-01') AS NEXT_SOURCE_FILE_DATE \n",
    "#                             FROM pricing_analytics.processrunlogs.deltalakehouse_process_runs\n",
    "#                             WHERE PROCESS_NAME = 'dailyPricingSourceIngest' AND PROCESS_STATUS = 'Completed'\"\"\"\n",
    "# nextSourceFileDateDF = spark.sql(nextSourceFileDateSQL)\n",
    "# next_source_file_date = datetime.strptime(\n",
    "#     str(nextSourceFileDateDF.select(\"NEXT_SOURCE_FILE_DATE\").collect()[0][\"NEXT_SOURCE_FILE_DATE\"]), \n",
    "#     \"%Y-%m-%d\"\n",
    "# )\n",
    "\n",
    "# # Base URL and folder settings\n",
    "# dailyPricingSourceBaseURL = \"https://retailpricing.blob.core.windows.net/\"\n",
    "# dailyPricingSubFolder = \"daily-pricing/\"\n",
    "# dailyPricingSinkLayerName = \"bronze\"\n",
    "# dailyPricingStorageAccountName = \"lckudadatalakehousedev\"\n",
    "# dailyPricingSinkFolderName = \"daily-pricing\"\n",
    "\n",
    "# # Loop through dates until end_date (2023-06-30)\n",
    "# while next_source_file_date <= end_date:\n",
    "#     dailyPricingSourceFileDate = next_source_file_date.strftime('%m%d%Y')\n",
    "#     dailyPricingSourceFileName = f\"PW_MW_DR_{dailyPricingSourceFileDate}.csv\"\n",
    "#     dailyPricingSourceURL = dailyPricingSourceBaseURL + dailyPricingSubFolder + dailyPricingSourceFileName\n",
    "#     dailyPricingSinkFolderPath = f\"abfss://{dailyPricingSinkLayerName}@{dailyPricingStorageAccountName}.dfs.core.windows.net/{dailyPricingSinkFolderName}\"\n",
    "\n",
    "#     try:\n",
    "#         print(f\"Processing file for date: {next_source_file_date.strftime('%Y-%m-%d')}\")\n",
    "#         # Read CSV from source URL\n",
    "#         dailyPricingPandasDF = pd.read_csv(dailyPricingSourceURL)\n",
    "#         dailyPricingSparkDF = spark.createDataFrame(dailyPricingPandasDF)\n",
    "#         dailyPricingSparkDF = dailyPricingSparkDF.withColumn(\"source_file_load_date\", current_timestamp())\n",
    "\n",
    "#         # Save to bronze layer (CSV)\n",
    "#         print(f\"Found {dailyPricingSourceURL}\")\n",
    "#         print(f\"Starting save data to bronze layer\")\n",
    "#         (\n",
    "#             dailyPricingSparkDF\n",
    "#             .write\n",
    "#             .mode(\"append\")\n",
    "#             .option(\"header\", \"true\")\n",
    "#             .csv(dailyPricingSinkFolderPath)\n",
    "#         )\n",
    "\n",
    "#         # Insert into bronze table\n",
    "#         (\n",
    "#             dailyPricingSparkDF\n",
    "#             .write\n",
    "#             .mode(\"append\")\n",
    "#             .insertInto(\"pricing_analytics.bronze.daily_pricing\")\n",
    "#         )\n",
    "\n",
    "#         # Log the process as Completed\n",
    "#         processStatus = 'Completed'\n",
    "#         processInsertSql = f\"\"\"INSERT INTO pricing_analytics.processrunlogs.DELTALAKEHOUSE_PROCESS_RUNS\n",
    "#                                (PROCESS_NAME, PROCESSED_FILE_TABLE_DATE, PROCESS_STATUS) \n",
    "#                                VALUES('{processName}', '{next_source_file_date.strftime('%Y-%m-%d')}', '{processStatus}')\"\"\"\n",
    "#         spark.sql(processInsertSql)\n",
    "#         print(f\"Successfully processed and logged date: {next_source_file_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Not Found {dailyPricingSourceURL}\")\n",
    "#         print(f\"Error: {str(e)}\")\n",
    "#         # Log the process as Completed even if file not found (as per original logic)\n",
    "#         processStatus = 'Completed'\n",
    "#         processInsertSql = f\"\"\"INSERT INTO pricing_analytics.processrunlogs.DELTALAKEHOUSE_PROCESS_RUNS\n",
    "#                                (PROCESS_NAME, PROCESSED_FILE_TABLE_DATE, PROCESS_STATUS) \n",
    "#                                VALUES('{processName}', '{next_source_file_date.strftime('%Y-%m-%d')}', '{processStatus}')\"\"\"\n",
    "#         spark.sql(processInsertSql)\n",
    "#         print(f\"Logged date as Completed despite error: {next_source_file_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "#     # Increment to the next date\n",
    "#     next_source_file_date += timedelta(days=1)\n",
    "\n",
    "# print(\"Completed processing all dates up to 2023-06-30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "888af8bc-f333-476b-876b-1786fe225801",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    dailyPricingSourceURL = dailyPricingSourceBaseURL + dailyPricingSubFolder + dailyPricingSourceFileName\n",
    "    dailyPricingPandasDF = pd.read_csv(dailyPricingSourceURL)\n",
    "    dailyPricingSparkDF = spark.createDataFrame(dailyPricingPandasDF)\n",
    "    dailyPricingSinkFolderPath = f\"abfss://{dailyPricingSinkLayerName}@{dailyPricingStorageAccountName}.dfs.core.windows.net/{dailyPricingSinkFolderName}\"\n",
    "    print(f\"Found {dailyPricingSourceURL}\")\n",
    "    print(f\"Starting save data to bronze layer\")\n",
    "    dailyPricingSparkDF = dailyPricingSparkDF.withColumn(\"source_file_load_date\", current_timestamp())\n",
    "\n",
    "    (\n",
    "        dailyPricingSparkDF.\n",
    "        write.\n",
    "        mode(\"append\").\n",
    "        option(\"header\", \"true\").\n",
    "        csv(dailyPricingSinkFolderPath)\n",
    "    )\n",
    "    \n",
    "    (\n",
    "        dailyPricingSparkDF.\n",
    "        write.\n",
    "        mode(\"append\").\n",
    "        insertInto(\"pricing_analytics.bronze.daily_pricing\")\n",
    "    )\n",
    "\n",
    "    processName = dbutils.widgets.get(\"prm_processName\")\n",
    "    processFileDate = nextSourceFileDateDF.select(\"NEXT_SOURCE_FILE_DATE\").collect()[0][\"NEXT_SOURCE_FILE_DATE\"]\n",
    "    processStatus ='Completed'\n",
    "\n",
    "    processInsertSql = f\"\"\" INSERT INTO pricing_analytics.processrunlogs.DELTALAKEHOUSE_PROCESS_RUNS(PROCESS_NAME,PROCESSED_FILE_TABLE_DATE,PROCESS_STATUS) VALUES('{processName}','{processFileDate}','{processStatus}')\"\"\"\n",
    "    spark.sql(processInsertSql)\n",
    "except Exception as e:\n",
    "    print(f\"Not Found {dailyPricingSourceURL}\")\n",
    "    print(f\"Starting Insert Date To Process Table\")\n",
    "    processName = dbutils.widgets.get(\"prm_processName\")\n",
    "    processFileDate = nextSourceFileDateDF.select(\"NEXT_SOURCE_FILE_DATE\").collect()[0][\"NEXT_SOURCE_FILE_DATE\"]\n",
    "    processStatus ='Completed'\n",
    "\n",
    "    processInsertSql = f\"\"\" INSERT INTO pricing_analytics.processrunlogs.DELTALAKEHOUSE_PROCESS_RUNS(PROCESS_NAME,PROCESSED_FILE_TABLE_DATE,PROCESS_STATUS) VALUES('{processName}','{processFileDate}','{processStatus}')\"\"\"\n",
    "    spark.sql(processInsertSql)\n",
    "\n",
    "# dailyPricingSparkDF = spark.createDataFrame(dailyPricingPandasDF)\n",
    "# dailyPricingSinkFolderPath = f\"abfss://{dailyPricingSinkLayerName}@{dailyPricingStorageAccountName}.dfs.core.windows.net/{dailyPricingSinkFolderName}\"\n",
    "\n",
    "# (\n",
    "#     dailyPricingSparkDF.\n",
    "#     withColumn(\"source_file_load_date\", current_timestamp()).\n",
    "#     write.\n",
    "#     mode(\"append\").\n",
    "#     option(\"header\", \"true\").\n",
    "#     csv(dailyPricingSinkFolderPath)\n",
    "# )\n",
    "\n",
    "# processName = dbutils.widgets.get(\"prm_processName\")\n",
    "# processFileDate = nextSourceFileDateDF.select(\"NEXT_SOURCE_FILE_DATE\").collect()[0][\"NEXT_SOURCE_FILE_DATE\"]\n",
    "# processStatus ='Completed'\n",
    "\n",
    "# processInsertSql = f\"\"\" INSERT INTO pricing_analytics.processrunlogs.DELTALAKEHOUSE_PROCESS_RUNS(PROCESS_NAME,PROCESSED_FILE_TABLE_DATE,PROCESS_STATUS) VALUES('{processName}','{processFileDate}','{processStatus}')\"\"\"\n",
    "# spark.sql(processInsertSql)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5737309449198467,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingest-Daily-Pricing-HTTP-Source-Data",
   "widgets": {
    "prm_processName": {
     "currentValue": "dailyPricingSourceIngest",
     "nuid": "d563eb9c-347b-4209-ad53-5fee99999cc8",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "prm_processName",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "prm_processName",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
